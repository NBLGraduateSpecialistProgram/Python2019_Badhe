{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "Data_Science_Part2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kDdCC8Mxdvki"
      },
      "source": [
        "## 1. Decision Tree "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nwhs2Ry_dvkj"
      },
      "source": [
        "Decision tree is the most popular classification techniques. Decision trees are used in everyday life decisions, not just in machine learning. Flow diagrams are actually visual representations of decision trees. \n",
        "\n",
        "Decision trees are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero-in on the classification.\n",
        "For example, if you wanted to build a decision tree to classify an animal you come across while on a hike, you might construct the one shown here:\n",
        "\n",
        "\n",
        "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.08-decision-tree.png?raw=1)\n",
        "\n",
        "\n",
        "The binary splitting makes this extremely efficient: in a well-constructed tree, each question will cut the number of options by approximately half, very quickly narrowing the options even among a large number of classes.\n",
        "The trick, of course, comes in deciding which questions to ask at each step.\n",
        "In machine learning implementations of decision trees, the questions generally take the form of axis-aligned splits in the data: that is, each node in the tree splits the data into two groups using a cutoff value within one of the features.\n",
        "\n",
        "\n",
        "### Construction of Decision Tree\n",
        "\n",
        "#### Spliting criteria\n",
        "**Entropy** : A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogeneous). If the sample is completely homogeneous the entropy is zero and if the sample is equally divided then it has entropy of one.\n",
        "\n",
        "\n",
        "![alt text](https://github.com/SankBad/GraduateSpecialistRutgers/blob/master/img3_entropy.png?raw=true)\n",
        "\n",
        "![alt text](https://github.com/SankBad/GraduateSpecialistRutgers/blob/master/0_sNMN2eKjMOHhNMZy.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "**Information Gain**: The information gain is based on the decrease in entropy after a data-set is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest information gain\n",
        "\n",
        "\n",
        "\n",
        "1.   Calculate entropy of the target.\n",
        "2.   The dataset is then split on the different attributes. The entropy for each branch is calculated. Then it is added proportionally, to get total entropy for the split. The resulting entropy is subtracted from the entropy before the split. The result is the Information Gain, or decrease in entropy.\n",
        "3.   Choose attribute with the largest information gain as the decision node, divide the dataset by its branches and repeat the same process on every branch.\n",
        "\n",
        "\n",
        "![alt text](https://github.com/SankBad/GraduateSpecialistRutgers/blob/master/img6_decis.png?raw=true)\n",
        "\n",
        "\n",
        "![alt text](https://github.com/SankBad/GraduateSpecialistRutgers/blob/master/img7_decis.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Gini Index**: Gini index says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure. But what is actually meant by ‘purity’? If all the elements belong to a single class, then it can be called pure.  The degree of Gini index varies between 0 and 1, where 0 denotes that all elements belong to a certain class or if there exists only one class, and 1 denotes that the elements are randomly distributed across various classes. A Gini Index of 0.5 denotes equally distributed elements into some classes.\n",
        "\n",
        "Steps to Calculate Gini for a split:\n",
        "1.   Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure 1 - (p²+q²).\n",
        "2.   Calculate Gini for split using weighted Gini score of each node of that split\n",
        "3.   Pick the best gini gain attribute.\n",
        "\n",
        "\n",
        "Example: — Referring to example where we want to segregate the students based on target variable ( playing cricket or not ). In the snapshot below, we split the population using two input variables Gender and Class. Now, I want to identify which split is producing more homogeneous sub-nodes using Gini index.\n",
        "\n",
        "\n",
        "![alt text](https://github.com/SankBad/GraduateSpecialistRutgers/blob/master/img2_gini.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "Split on Gender:\n",
        "Gini for sub-node Female = 1 - (0.2)*(0.2)-(0.8)*(0.8)=0.32\n",
        "Gini for sub-node Male = 1 - (0.65)*(0.65)-(0.35)*(0.35)=0.45\n",
        "Weighted Gini for Split Gender = (10/30)*0.32+(20/30)*0.45 = 0.40\n",
        "Similar for Split on Class:\n",
        "Gini for sub-node Class IX = 1 - (0.43)*(0.43)-(0.57)*(0.57)=0.49\n",
        "Gini for sub-node Class X = 1 - (0.56)*(0.56)-(0.44)*(0.44)=0.49\n",
        "Weighted Gini for Split Class = (14/30)*0.49+(16/30)*0.49 = 0.49\n",
        "\n",
        "Above, you can see that Gini score for Split on Gender is lower than Split on Class, hence, the node split will take place on Gender.\n",
        "\n",
        "\n",
        "\n",
        "**Reduction in Variance** : Use for regression \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z1gr1PCxdvkl",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XjU7R6Iudvkv"
      },
      "source": [
        "\n",
        "\n",
        "Consider the following two-dimensional data, which has one of four class labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sSL8NGtCdvkx",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=4,\n",
        "                  random_state=0, cluster_std=1.0)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IsjpIR66dvk3"
      },
      "source": [
        "A simple decision tree built on this data will iteratively split the data along one or the other axis according to some quantitative criterion, and at each level assign the label of the new region according to a majority vote of points within it.\n",
        "This figure presents a visualization of the first four levels of a decision tree classifier for this data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oLSDPZoMdvk6"
      },
      "source": [
        "Notice that after the first split, every point in the upper branch remains unchanged, so there is no need to further subdivide this branch.\n",
        "\n",
        "![](https://github.com/SankBad/GraduateSpecialistRutgers/blob/master/img5_decis.png?raw=true)\n",
        "\n",
        "Except for nodes that contain all of one color, at each level *every* region is again split along one of the two features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VNl9eEgwdvk9"
      },
      "source": [
        "This process of fitting a decision tree to our data can be done in Scikit-Learn with the ``DecisionTreeClassifier`` estimator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y3mOAaJQdvk_",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree = DecisionTreeClassifier().fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6UZoWl4IdvlH"
      },
      "source": [
        "Let's write a quick utility function to help us visualize the output of the classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GtzIDYJTdvlJ",
        "colab": {}
      },
      "source": [
        "def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n",
        "    ax = ax or plt.gca()\n",
        "    \n",
        "    # Plot the training points\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
        "               clim=(y.min(), y.max()), zorder=3)\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    \n",
        "    # fit the estimator\n",
        "    model.fit(X, y)\n",
        "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
        "                         np.linspace(*ylim, num=200))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "    # Create a color plot with the results\n",
        "    n_classes = len(np.unique(y))\n",
        "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
        "                           levels=np.arange(n_classes + 1) - 0.5,\n",
        "                           cmap=cmap, clim=(y.min(), y.max()),\n",
        "                           zorder=1)\n",
        "\n",
        "    ax.set(xlim=xlim, ylim=ylim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MAJa_Y0TdvlO"
      },
      "source": [
        "Now we can examine what the decision tree classification looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4-uTPcpqdvlP",
        "colab": {}
      },
      "source": [
        "visualize_classifier(DecisionTreeClassifier(), X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qS6-OSHIdvle"
      },
      "source": [
        "Notice that as the depth increases, we tend to get very strangely shaped classification regions; for example, at a depth of five, there is a tall and skinny purple region between the yellow and blue regions.\n",
        "It's clear that this is less a result of the true, intrinsic data distribution, and more a result of the particular sampling or noise properties of the data.\n",
        "That is, this decision tree, even at only five levels deep, is clearly over-fitting our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D_VfknVHdvlg"
      },
      "source": [
        "### Decision trees and over-fitting\n",
        "\n",
        "Such over-fitting turns out to be a general property of decision trees: it is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions they are drawn from.\n",
        "Another way to see this over-fitting is to look at models trained on different subsets of the data—for example, in this figure we train two different trees, each on half of the original data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PR-N9EW9dvlh"
      },
      "source": [
        "![](https://github.com/SankBad/GraduateSpecialistRutgers/blob/master/img4_decis.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qd12X09sdvli"
      },
      "source": [
        "It is clear that in some places, the two trees produce consistent results (e.g., in the four corners), while in other places, the two trees give very different classifications (e.g., in the regions between any two clusters).\n",
        "The key observation is that the inconsistencies tend to happen where the classification is less certain, and thus by using information from *both* of these trees, we might come up with a better result!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GwjLPUHLdvlp"
      },
      "source": [
        "Just as using information from two trees improves our results, we might expect that using information from many trees would improve our results even further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xSzQ21T8dvlq"
      },
      "source": [
        "## Ensembles of Estimators: Random Forests\n",
        "\n",
        "Bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. (Wikipedia)\n",
        "\n",
        "This notion—that multiple overfitting estimators can be combined to reduce the effect of this overfitting—is what underlies an ensemble method called *bagging*.\n",
        "Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of which over-fits the data, and averages the results to find a better classification.\n",
        "An ensemble of randomized decision trees is known as a *random forest*.\n",
        "\n",
        "\n",
        "![alt text](https://github.com/SankBad/GraduateSpecialistRutgers/blob/master/img9_decis.png?raw=true)\n",
        "\n",
        "This type of bagging classification can be done manually using Scikit-Learn's ``BaggingClassifier`` meta-estimator, as shown here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lUMdOoT5dvls",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier()\n",
        "bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,\n",
        "                        random_state=1)\n",
        "\n",
        "bag.fit(X, y)\n",
        "visualize_classifier(bag, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8N_LV_fAdvly"
      },
      "source": [
        "In this example, we have randomized the data by fitting each estimator with a random subset of 80% of the training points.\n",
        "In practice, decision trees are more effectively randomized by injecting some stochasticity in how the splits are chosen: this way all the data contributes to the fit each time, but the results of the fit still have the desired randomness.\n",
        "\n",
        "i.e.\n",
        "Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of the tree, only a random subset of features are considered for each split.\n",
        "\n",
        "For example, when determining which feature to split on, the randomized tree might select from among the top several features.\n",
        "You can read more technical details about these randomization strategies in the [Scikit-Learn documentation](http://scikit-learn.org/stable/modules/ensemble.html#forest) and references within.\n",
        "\n",
        "In Scikit-Learn, such an optimized ensemble of randomized decision trees is implemented in the ``RandomForestClassifier`` estimator, which takes care of all the randomization automatically.\n",
        "All you need to do is select a number of estimators, and it will very quickly (in parallel, if desired) fit the ensemble of trees:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ymGyyrTVdvl0",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "visualize_classifier(model, X, y);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AQvPCRWKdvl5"
      },
      "source": [
        "We see that by averaging over 100 randomly perturbed models, we end up with an overall model that is much closer to our intuition about how the parameter space should be split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0gl41H5MdvmF"
      },
      "source": [
        "Here the true model is shown in the smooth gray curve, while the random forest model is shown by the jagged red curve.\n",
        "As you can see, the non-parametric random forest model is flexible enough to fit the multi-period data, without us needing to specifying a multi-period model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9dxvB9ITdvmH"
      },
      "source": [
        "## Example: Random Forest for Classifying Digits\n",
        "\n",
        "\n",
        "Let's use random forest classifier for classifying digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RD642895dvmJ",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "digits.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ncPsWvcDdvmO"
      },
      "source": [
        "lets visualize the first few data points:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YkqycZrWdvmQ",
        "colab": {}
      },
      "source": [
        "# set up the figure\n",
        "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "# plot the digits: each image is 8x8 pixels\n",
        "for i in range(64):\n",
        "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
        "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
        "    \n",
        "    # label the image with the target value\n",
        "    ax.text(0, 7, str(digits.target[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ih8FJrNEdvmU"
      },
      "source": [
        "We can quickly classify the digits using a random forest as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hCPl0aindvmW",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target, random_state=0)\n",
        "model = RandomForestClassifier(n_estimators=1000)\n",
        "model.fit(Xtrain, ytrain)\n",
        "ypred = model.predict(Xtest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PKYo5J_Odvmb"
      },
      "source": [
        "We can take a look at the classification report for this classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aMhoPo6mdvmc",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(ypred, ytest))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I5b3DPGUdvmg"
      },
      "source": [
        "And for good measure, plot the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yZPUeyJLdvmi",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "mat = confusion_matrix(ytest, ypred)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QYoZHNWnl88I"
      },
      "source": [
        "## Support Vector Machines\n",
        "\n",
        "lets create data first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K25_BdigyrZh",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "# use seaborn plotting defaults\n",
        "import seaborn as sns; sns.set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L9tCirCcyNJA",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "X, y = make_blobs(n_samples=50, centers=2,\n",
        "                  random_state=0, cluster_std=0.60)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XKJInZYazIjx"
      },
      "source": [
        "A linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification.\n",
        "For two dimensional data like that shown here, this is a task we could do by hand.\n",
        "But immediately we see a problem: there is more than one possible dividing line that can perfectly discriminate between the two classes!\n",
        "\n",
        "We can draw them as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "18eCnDLdzNx0",
        "colab": {}
      },
      "source": [
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
        "\n",
        "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
        "    plt.plot(xfit, m * xfit + b, '-k')\n",
        "\n",
        "plt.xlim(-1, 3.5);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JI7yBchTz-za"
      },
      "source": [
        "These are three *very* different separators which, nevertheless, perfectly discriminate between these samples.\n",
        "Depending on which you choose, a new data point (e.g., the one marked by the \"X\" in this plot) will be assigned a different label!\n",
        "Evidently our simple intuition of \"drawing a line between classes\" is not enough, and we need to think a bit deeper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kEM2-6eD0F3A"
      },
      "source": [
        "## Support Vector Machines: Maximizing the *Margin*\n",
        "\n",
        "Support vector machines offer one way to improve on this.\n",
        "The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a *margin* of some width, up to the nearest point.\n",
        "Here is an example of how this might look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HH4z81Rzzmgp",
        "colab": {}
      },
      "source": [
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "\n",
        "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
        "    yfit = m * xfit + b\n",
        "    plt.plot(xfit, yfit, '-k')\n",
        "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
        "                     color='#AAAAAA', alpha=0.4)\n",
        "\n",
        "plt.xlim(-1, 3.5);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2nih2hfj1Cqo"
      },
      "source": [
        "In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model.\n",
        "Support vector machines are an example of such a *maximum margin* estimator.\n",
        "\n",
        "### Fitting a support vector machine\n",
        "\n",
        "Let's see the result of an actual fit to this data: we will use Scikit-Learn's support vector classifier to train an SVM model on this data.\n",
        "For the time being, we will use a linear kernel and set the ``C`` parameter to a very large number (we'll discuss the meaning of these in more depth momentarily)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dNec7NTM1KCd",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"\n",
        "model = SVC(kernel='linear', C=1E10)\n",
        "model.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YrV5AjCL1dtC"
      },
      "source": [
        "To better visualize what's happening here, let's create a quick convenience function that will plot SVM decision boundaries for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pM0dCG9p1jj3",
        "colab": {}
      },
      "source": [
        "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    \n",
        "    # create grid to evaluate model\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
        "    P = model.decision_function(xy).reshape(X.shape)\n",
        "    \n",
        "    # plot decision boundary and margins\n",
        "    ax.contour(X, Y, P, colors='k',\n",
        "               levels=[-1, 0, 1], alpha=0.5,\n",
        "               linestyles=['--', '-', '--'])\n",
        "    \n",
        "    # plot support vectors\n",
        "    if plot_support:\n",
        "        ax.scatter(model.support_vectors_[:, 0],\n",
        "                   model.support_vectors_[:, 1],\n",
        "                   s=300, linewidth=1, facecolors='none');\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h8BPMA0q1puM",
        "colab": {}
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(model);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EBl4dzFY17oz"
      },
      "source": [
        "This is the dividing line that maximizes the margin between the two sets of points.\n",
        "Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure.\n",
        "These points are the pivotal elements of this fit, and are known as the *support vectors*, and give the algorithm its name.\n",
        "In Scikit-Learn, the identity of these points are stored in the ``support_vectors_`` attribute of the classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HVs5ZD4z13Nj",
        "colab": {}
      },
      "source": [
        "model.support_vectors_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sp2Ir9OZ6Fn9"
      },
      "source": [
        "A key to this classifier's success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit!\n",
        "Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\n",
        "\n",
        "We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SD0QVesO6HmG",
        "colab": {}
      },
      "source": [
        "def plot_svm(N=10, ax=None):\n",
        "    X, y = make_blobs(n_samples=200, centers=2,\n",
        "                      random_state=0, cluster_std=0.60)\n",
        "    X = X[:N]\n",
        "    y = y[:N]\n",
        "    model = SVC(kernel='linear', C=1E10)\n",
        "    model.fit(X, y)\n",
        "    \n",
        "    ax = ax or plt.gca()\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "    ax.set_xlim(-1, 4)\n",
        "    ax.set_ylim(-1, 6)\n",
        "    plot_svc_decision_function(model, ax)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "for axi, N in zip(ax, [60, 120]):\n",
        "    plot_svm(N, axi)\n",
        "    axi.set_title('N = {0}'.format(N))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xMaadBX-6oUg"
      },
      "source": [
        "you can use IPython's interactive widgets to view this feature of the SVM model interactively:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HyuMhlE66ao9",
        "colab": {}
      },
      "source": [
        "from ipywidgets import interact, fixed\n",
        "interact(plot_svm, N=[10, 50, 60, 70, 200], ax=fixed(None));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t9Uq0wVL6w30"
      },
      "source": [
        "### Beyond linear boundaries: Kernel SVM\n",
        "\n",
        "Where SVM becomes extremely powerful is when it is combined with *kernels*.\n",
        "In SVM models, we can use kernal to project our data into higher-dimensional space defined by polynomials and Gaussian basis functions, and thereby were able to fit for nonlinear relationships with a linear classifier.\n",
        "To motivate the need for kernels, let's look at some data that is not linearly separable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DBXYG68e7OUP",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets.samples_generator import make_circles\n",
        "X, y = make_circles(100, factor=.1, noise=.1)\n",
        "\n",
        "clf = SVC(kernel='linear').fit(X, y)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(clf, plot_support=False);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UAcctW5D7VMS"
      },
      "source": [
        "No linear discrimination will *ever* be able to separate this data.\n",
        "But we might project the data into a higher dimension such that a linear separator *would* be sufficient.\n",
        "For example, one simple projection we could use would be to compute a *radial basis function* centered on the middle clump:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ro9dEU417eXs",
        "colab": {}
      },
      "source": [
        "r = np.exp(-(X ** 2).sum(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2NC09cNs7oFY"
      },
      "source": [
        "We can visualize this extra data dimension using a three-dimensional plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VQkjK_jj9vQ3",
        "colab": {
          "referenced_widgets": [
            "61614b5e9bbf40a7823b8ac530ecd2bf"
          ]
        },
        "outputId": "5a950265-1fea-4e99-c1e9-fbe15a5e1e69"
      },
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "def plot_3D(elev=30, azim=30, X=X, y=y):\n",
        "    ax = plt.subplot(projection='3d')\n",
        "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
        "    ax.view_init(elev=elev, azim=azim)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_zlabel('r')\n",
        "\n",
        "interact(plot_3D, elev=[-90, -50, 0, 20, 40,  70, 45,  90], azip=(-180, 180),\n",
        "         X=fixed(X), y=fixed(y));"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61614b5e9bbf40a7823b8ac530ecd2bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='elev', options=(-90, -50, 0, 20, 40, 70, 45, 90), value=-90), IntS…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "smictr9p_yVQ",
        "colab": {}
      },
      "source": [
        "clf = SVC(kernel='rbf', C=1E6)\n",
        "clf.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m-eK5PnV_0LW",
        "colab": {}
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(clf)\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "            s=300, lw=1, facecolors='none');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4NLab1g5AFWn"
      },
      "source": [
        "Using this kernelized support vector machine, we learn a suitable nonlinear decision boundary.\n",
        "This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gxv5xAQpAMFa"
      },
      "source": [
        "### Tuning the SVM: Soft Margins\n",
        "\n",
        "Our discussion thus far has centered around very clean datasets, in which a perfect decision boundary exists.\n",
        "But what if your data has some amount of overlap?\n",
        "For example, you may have data like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "801_8EsOARSn",
        "colab": {}
      },
      "source": [
        "X, y = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=0, cluster_std=1.2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mhbaWyQYAilt"
      },
      "source": [
        "To handle this case, the SVM implementation has a bit of a fudge-factor which \"softens\" the margin: that is, it allows some of the points to creep into the margin if that allows a better fit.\n",
        "The hardness of the margin is controlled by a tuning parameter, most often known as $C$.\n",
        "For very large $C$, the margin is hard, and points cannot lie in it.\n",
        "For smaller $C$, the margin is softer, and can grow to encompass some points.\n",
        "\n",
        "The plot shown below gives a visual picture of how a changing $C$ parameter affects the final fit, via the softening of the margin:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qoMm3g_MAhWx",
        "colab": {}
      },
      "source": [
        "X, y = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=0, cluster_std=0.8)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "\n",
        "for axi, C in zip(ax, [10.0, 0.1]):\n",
        "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
        "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "    plot_svc_decision_function(model, axi)\n",
        "    axi.scatter(model.support_vectors_[:, 0],\n",
        "                model.support_vectors_[:, 1],\n",
        "                s=300, lw=1, facecolors='none');\n",
        "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TQjy2ZpYAvDD"
      },
      "source": [
        "The optimal value of the $C$ parameter will depend on your dataset, and should be tuned using cross-validation or a similar procedure \n",
        "\n",
        "Refrence: Data Science with python textbook"
      ]
    }
  ]
}