{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Data-Science-with-Python-part1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1e2bgLB2VZd",
        "colab_type": "text"
      },
      "source": [
        "## Machine learning with scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeYeg8RE2VZi",
        "colab_type": "text"
      },
      "source": [
        "*The following materials (including text, code, and figures) were adapted from the \"SciPy 2017 Scikit-learn Tutorial\" by Alexandre Gramfort and Andreas Mueller. The contents of their tutorial are licensed under Creative Commons CC0 1.0 Universal License as work dedicated to the public domain, and can be found at https://github.com/amueller/scipy-2017-sklearn.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ3S9hLy2VZk",
        "colab_type": "text"
      },
      "source": [
        "## What is Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQtwEiqa2VZm",
        "colab_type": "text"
      },
      "source": [
        "Machine learning is the process of extracting knowledge from data automatically, usually with the goal of making predictions on new, unseen data. Put another way, you are giving samples of data to the machine, which tries to infer observations from the data.\n",
        "\n",
        "Two key concepts:\n",
        "- **automating decision making** from data **without the user specifying explicit rules** for how this decision should be made\n",
        "- **generalization**: the goal of a machine learning model is to predict on new, previously unseen data\n",
        "\n",
        "The data is usually presented to the algorithm as a two-dimensional array (or matrix) of numbers. Each data point (also known as a *sample* or *training instance*) is represented as a list of numbers, a so-called feature vector, and the features that comprise the vector represent the properties of this point. \n",
        "\n",
        "For instance, we can represent a dataset consisting of 150 samples and 4 features as a 2-dimensional array or matrix $\\mathbb{R}^{150 \\times 4}$ in the following format:\n",
        "\n",
        "\n",
        "$$\\mathbf{X} = \\begin{bmatrix}\n",
        "    x_{1}^{(1)} & x_{2}^{(1)} & x_{3}^{(1)} & \\dots  & x_{4}^{(1)} \\\\\n",
        "    x_{1}^{(2)} & x_{2}^{(2)} & x_{3}^{(2)} & \\dots  & x_{4}^{(2)} \\\\\n",
        "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    x_{1}^{(150)} & x_{2}^{(150)} & x_{3}^{(150)} & \\dots  & x_{4}^{(150)}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "(The superscript denotes the *i*th row, and the subscript denotes the *j*th feature, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-uAF4zH2VZn",
        "colab_type": "text"
      },
      "source": [
        "Data in scikit-learn, with very few exceptions, is assumed to be stored as a\n",
        "**two-dimensional array**, of shape `[n_samples, n_features]`.\n",
        "\n",
        "- **n_samples:**   The number of samples: each sample is an item to process (e.g. classify).\n",
        "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
        "  a row in database or CSV file,\n",
        "  or whatever you can describe with a fixed set of quantitative traits.\n",
        "- **n_features:**  The number of features or distinct traits that can be used to describe each\n",
        "  item in a quantitative manner.  Features are generally real-valued, but may be Boolean or\n",
        "  discrete-valued in some cases.\n",
        "\n",
        "The number of features must be fixed in advance. However it can be very high dimensional\n",
        "(e.g. millions of features) with most of them being \"zeros\" for a given sample. This is a case\n",
        "where `scipy.sparse` matrices can be useful, in that they are\n",
        "much more memory-efficient than NumPy arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h11i3M1d2VZp",
        "colab_type": "text"
      },
      "source": [
        "There are two kinds of machine learning we will talk about today: ***supervised learning*** and ***unsupervised learning***."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln_l5Gek2VZq",
        "colab_type": "text"
      },
      "source": [
        "### Supervised Learning: Classification and regression\n",
        "\n",
        "In **Supervised Learning**, we have a dataset consisting of both input features (observed quantities) and a desired output (what we want to determine).\n",
        "\n",
        "Some examples are:\n",
        "\n",
        "- Given a photograph of a person, identify the person in the photo.\n",
        "- Given a list of movies a person has watched and their personal ratings\n",
        "  of the movies, recommend a list of movies they would like.\n",
        "- Given a persons age, education and position, infer their salary.\n",
        "\n",
        "Supervised learning is further broken down into two categories, **classification** and **regression**:\n",
        "\n",
        "- **In classification, the label is discrete**, such as \"spam\" or \"no spam\" for an email. \n",
        "\n",
        "- **In regression, the label is continuous** (a float output). \n",
        "\n",
        "In supervised learning, there is always a distinction between a **training set** for which the desired outcome (a certain label or class) is given, and a **test set** for which the desired outcome needs to be inferred. The learning model fits the predictive model to the training set, and we use the test set to evaluate its generalization performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbF13-cR2VZs",
        "colab_type": "text"
      },
      "source": [
        "### Unsupervised Learning\n",
        "\n",
        "In **Unsupervised Learning** there is no desired output associated with the data.\n",
        "Instead, we are interested in extracting some form of knowledge or model from the given data.\n",
        "In a sense, you can think of unsupervised learning as a means of discovering labels from the data itself.\n",
        "\n",
        "Unsupervised learning comprises tasks such as *dimensionality reduction*, *clustering*, and\n",
        "*anomaly detection*. Some unsupervised learning problems are:\n",
        "\n",
        "- Given detailed observations of distant galaxies, determine which features or combinations of\n",
        "  features best summarize the information.\n",
        "- Given a large collection of news articles, find recurring topics inside these articles.\n",
        "- Given a video, isolate a moving object and categorize in relation to other moving objects which have been seen.\n",
        "\n",
        "Sometimes the two types of learning may even be combined: e.g. unsupervised learning can be used to find useful\n",
        "features in heterogeneous data, and then these features can be used within a supervised\n",
        "framework.\n",
        "\n",
        "\n",
        "#### Breif overview of machine learning taxonomy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrYUliMv2VZu",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/2552/1*qYrLCg4h2NVXFNw424rKIQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il_cmyFH2VZv",
        "colab_type": "text"
      },
      "source": [
        "### A Simple Example: The Iris Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEc5Cm_X2VZx",
        "colab_type": "text"
      },
      "source": [
        "As an example of a simple dataset, we're going to take a look at the iris dataset stored by scikit-learn.\n",
        "The data consists of measurements of three different iris flower species.  There are three different species of iris\n",
        "in this particular dataset: Iris-Setosa, Iris-Versicolor, and Iris-Virginica.\n",
        "\n",
        "The data consist of the following:\n",
        "\n",
        "- Features in the Iris dataset:\n",
        "\n",
        "  1. sepal length in cm\n",
        "  2. sepal width in cm\n",
        "  3. petal length in cm\n",
        "  4. petal width in cm\n",
        "\n",
        "- Target classes to predict:\n",
        "\n",
        "  1. Iris Setosa\n",
        "  2. Iris Versicolour\n",
        "  3. Iris Virginica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQvH-2882VZy",
        "colab_type": "text"
      },
      "source": [
        "``scikit-learn`` embeds a copy of the iris CSV file along with a helper function to load it into numpy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rBmWHQg2VZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 1\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi377TpG2VZ6",
        "colab_type": "text"
      },
      "source": [
        "The resulting dataset is a ``Bunch`` object; you can see what's available using\n",
        "the method ``keys()``:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFFRvvz52VZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 2\n",
        "\n",
        "iris.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTR330dE2VaB",
        "colab_type": "text"
      },
      "source": [
        "The features of each sample flower are stored in the ``data`` attribute of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29QQOK692VaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 3\n",
        "\n",
        "n_samples, n_features = iris.data.shape\n",
        "print('Number of samples:', n_samples)\n",
        "print('Number of features:', n_features)\n",
        "# the sepal length, sepal width, petal length and petal width of the first sample (first flower)\n",
        "print(iris.data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FWO8J_n2VaI",
        "colab_type": "text"
      },
      "source": [
        "The information about the class of each sample is stored in the ``target`` attribute of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1TRDZsw2VaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 4\n",
        "\n",
        "print('Target array shape:', iris.target.shape)\n",
        "print('\\nTarget array:', iris.target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLaROEnM2VaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 5\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.bincount(iris.target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOBAZKqy2VaW",
        "colab_type": "text"
      },
      "source": [
        "Using the NumPy's bincount function (above), we can see that the classes are distributed uniformly in this dataset - there are 50 flowers from each species, where\n",
        "\n",
        "- class 0: Iris-Setosa\n",
        "- class 1: Iris-Versicolor\n",
        "- class 2: Iris-Virginica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYqeZzUy2VaY",
        "colab_type": "text"
      },
      "source": [
        "These class names are stored in the last attribute, namely ``target_names``:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ8LqA2e2Vaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 6\n",
        "\n",
        "print(iris.target_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtY5pfRH2Vai",
        "colab_type": "text"
      },
      "source": [
        "This data is four dimensional, but we can visualize one or two of the dimensions\n",
        "at a time using a simple histogram.  Again, we'll start by enabling\n",
        "matplotlib inline mode:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMQCdb2h2Vak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 7\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX0ZcjGK2Vas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 8\n",
        "\n",
        "x_index = 3\n",
        "colors = ['red', 'blue', 'magenta']\n",
        "\n",
        "for label, color in zip(range(len(iris.target_names)), colors):\n",
        "    plt.hist(iris.data[iris.target==label, x_index], \n",
        "             label=iris.target_names[label],\n",
        "             color=color)\n",
        "\n",
        "plt.xlabel(iris.feature_names[x_index])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ5JWsvF2Vax",
        "colab_type": "text"
      },
      "source": [
        "## Training and Testing Data\n",
        "\n",
        "To evaluate how well our supervised models generalize, we can split our data into a training and a test set. Below, we use 80% of the data for training, and 20% for testing. Other splits - such as 2/3 training and 1/3 test - could also be used. The most important thing is to fairly evaluate your system on data it *has not* seen during training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHpCIIyR2Vaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 9\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = iris.data, iris.target\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, \n",
        "                                                    train_size=0.8,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=123)\n",
        "print(\"Labels for training and testing data\")\n",
        "print(train_y)\n",
        "print(test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVqQGvKD2Va4",
        "colab_type": "text"
      },
      "source": [
        "**Tip: Stratified Split**\n",
        "\n",
        "Especially for relatively small datasets, it's better to stratify the split. Stratification means that we maintain the original class proportion of the dataset in the test and training sets. For example, after we randomly split the dataset as shown in the previous code example, we have the following class proportions in percent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-X4J__T2Va6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 10\n",
        "\n",
        "print('All:', np.bincount(y) / float(len(y)) * 100.0)\n",
        "print('Training:', np.bincount(train_y) / float(len(train_y)) * 100.0)\n",
        "print('Test:', np.bincount(test_y) / float(len(test_y)) * 100.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS-cdnbm2Va-",
        "colab_type": "text"
      },
      "source": [
        "So, in order to stratify the split, we can pass the label array as an additional option to the `train_test_split` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUbme-5H2VbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 11\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, \n",
        "                                                    train_size=0.8,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=123,\n",
        "                                                    stratify=y)\n",
        "\n",
        "print('All:', np.bincount(y) / float(len(y)) * 100.0)\n",
        "print('Training:', np.bincount(train_y) / float(len(train_y)) * 100.0)\n",
        "print('Test:', np.bincount(test_y) / float(len(test_y)) * 100.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI1K7WzU2VbF",
        "colab_type": "text"
      },
      "source": [
        "**Cross-validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHN4qqmR2VbH",
        "colab_type": "text"
      },
      "source": [
        "A common way to use more of the data to build a model, but also get a more robust estimate of the generalization performance, is cross-validation.\n",
        "In cross-validation, the data is split repeatedly into a training and non-overlapping test-sets, with a separate model built for every pair. The test-set scores are then aggregated for a more robust estimate.\n",
        "\n",
        "The most common way to do cross-validation is k-fold cross-validation, in which the data is first split into k (often 5 or 10) equal-sized folds, and then for each iteration, one of the k folds is used as test data, and the rest as training data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeFVsNYk2VbI",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://imada.sdu.dk/~marco/Teaching/AY2010-2011/DM825/animation.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDbJuRap2VbJ",
        "colab_type": "text"
      },
      "source": [
        "This way, each data point will be in the test-set exactly once, and we can use all but a k'th of the data for training. The ``sklearn.model_selection`` module has all functions related to cross validation. For example, we can use the Stratified K-Folds cross-validator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cttp9FZ2VbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 12\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dds0wCVE2VbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 13\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5)\n",
        "for train, test in cv.split(iris.data, iris.target):\n",
        "    print(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad9lyH272VbV",
        "colab_type": "text"
      },
      "source": [
        "As you can see, there are some samples from the beginning, some from the middle, and some from the end, in each of the folds.\n",
        "This way, the class ratios are preserved. Let's visualize the split:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofy2Jy6X2Vba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 14\n",
        "\n",
        "def plot_cv(cv, features, labels):\n",
        "    masks = []\n",
        "    for train, test in cv.split(features, labels):\n",
        "        mask = np.zeros(len(labels), dtype=bool)\n",
        "        mask[test] = 1\n",
        "        masks.append(mask)\n",
        "    \n",
        "    plt.matshow(masks, cmap='gray_r')\n",
        "    \n",
        "plot_cv(StratifiedKFold(n_splits=5), iris.data, iris.target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUZ-DgJB2Vbg",
        "colab_type": "text"
      },
      "source": [
        "For more information and to see other cross-validation techniques in scikit-learn, check out the documentation: http://scikit-learn.org/stable/modules/cross_validation.html."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHGzxOEICRVD",
        "colab_type": "text"
      },
      "source": [
        "## Supervised Learning: Linear Regression Example\n",
        "\n",
        "Linear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output). Hence, the name is Linear Regression. If we plot the independent variable (x) on the x-axis and dependent variable (y) on the y-axis, linear regression gives us a straight line that best fits the data points.\n",
        "\n",
        "**Simple linear Regression**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/864/0*QG8dIxNTaBH7Qrxq)\n",
        "\n",
        "What is the best fit?\n",
        "\n",
        "Reduce RSS. \n",
        "What is RSS? \n",
        "\n",
        "\n",
        "Multiple linear Regression\n",
        "\n",
        "![alt text](https://miro.medium.com/max/619/1*r3aOsJoXHX7uC2nxn2lygQ.png)\n",
        "\n",
        "![alt text](https://miro.medium.com/max/271/1*MKaaqL--30i1a4Y3zFVELw.png)\n",
        "\n",
        "![alt text](https://miro.medium.com/max/770/1*gc1jko6kNBj_R7QSSSn38w.png)\n",
        "\n",
        "![alt text](https://miro.medium.com/max/374/1*vdMGX4TS7Irs4ejDFudqjQ.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dELQFK-j11z2",
        "colab_type": "text"
      },
      "source": [
        "The Boston Housing dataset contains information about various houses in Boston through different parameters. \n",
        "There are 506 samples and 13 feature variables in this dataset. The objective is to predict the value of prices of the house using the given features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe3UQj1o2ZPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 15\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbbBFZ8S10-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 16\n",
        "#loading the dataset direclty from sklearn\n",
        "boston = datasets.load_boston()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVVCKRkr2cxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 17\n",
        "print(boston.keys())\n",
        "print('\\n')\n",
        "print(boston.data.shape)\n",
        "print('\\n')\n",
        "print(boston.feature_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhO0O8Zd2oVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 18\n",
        "print(boston.DESCR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtSsbTel2vqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 19\n",
        "bos = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
        "bos['PRICE'] = boston.target\n",
        "\n",
        "print(bos.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGWipXvw3DVE",
        "colab_type": "text"
      },
      "source": [
        "**Data preprocessing**\n",
        "After loading the data, it’s a good practice to see if there are any missing values in the data. We count the number of missing values for each feature using .isnull()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8qib5BX3JoS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "75198cf8-c6e3-4cab-bf50-1695be897493"
      },
      "source": [
        "## CODE CELL 20\n",
        "bos.isnull().sum()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRIM       0\n",
              "ZN         0\n",
              "INDUS      0\n",
              "CHAS       0\n",
              "NOX        0\n",
              "RM         0\n",
              "AGE        0\n",
              "DIS        0\n",
              "RAD        0\n",
              "TAX        0\n",
              "PTRATIO    0\n",
              "B          0\n",
              "LSTAT      0\n",
              "PRICE      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnYEaXDV3LqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(bos.describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7l1Q0QF38Fy",
        "colab_type": "text"
      },
      "source": [
        "It seems that your minimum price is larger than zero. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTNBxOZq3ZlN",
        "colab_type": "text"
      },
      "source": [
        "**Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EEOIlU53YoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(bos['PRICE']);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GO9xLSO48jZ",
        "colab_type": "text"
      },
      "source": [
        "*   Deviate from the normal distribution.\n",
        "*   Have appreciable positive skewness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wWW-RIN4cnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#histogram and normal probability plot\n",
        "sns.distplot(bos['PRICE'], fit=norm);\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(bos['PRICE'], plot=plt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JEKoMrG5XHj",
        "colab_type": "text"
      },
      "source": [
        "From above graph, 'Price' is not normal. It shows positive skewness and does not follow the diagonal line.\n",
        "\n",
        "Lets try log transformation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUQV44Mu5rEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bos['PRICE'] = np.log(bos['PRICE'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhyCv_MY50u2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#histogram and normal probability plot\n",
        "sns.distplot(bos['PRICE'], fit=norm);\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(bos['PRICE'], plot=plt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vnRug-I6va1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " sns.pairplot(bos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iwkY54_7Th-",
        "colab_type": "text"
      },
      "source": [
        "You can also use correlation matrix for same purpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7ynK4un7gbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correlation_matrix = bos.corr().round(2)\n",
        "sns.heatmap(data=correlation_matrix, annot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4MMhEY78esB",
        "colab_type": "text"
      },
      "source": [
        "Lets fit regression model with all the variable "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxk5omjz8daY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bb2d9b0f-3b23-4a3e-d278-3a950c72bb37"
      },
      "source": [
        "X = bos.drop('PRICE', axis = 1)\n",
        "y = bos['PRICE']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
        "\n",
        "reg_all = LinearRegression()\n",
        "reg_all.fit(X_train, y_train)\n",
        "\n",
        "# model evaluation for training set\n",
        "\n",
        "y_train_predict = reg_all.predict(X_train)\n",
        "rmse = (np.sqrt(mean_squared_error(y_train, y_train_predict)))\n",
        "r2 = round(reg_all.score(X_train, y_train),2)\n",
        "\n",
        "print(\"The model performance for training set\")\n",
        "print(\"--------------------------------------\")\n",
        "print('RMSE is {}'.format(rmse))\n",
        "print('R2 score is {}'.format(r2))\n",
        "print(\"\\n\")"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model performance for training set\n",
            "--------------------------------------\n",
            "RMSE is 0.1857714206727534\n",
            "R2 score is 0.8\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuUiw7Qv94Yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = reg_all.predict(X_test)\n",
        "rmse = (np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "r2 = round(reg_all.score(X_test, y_test),2)\n",
        "\n",
        "print(\"The model performance for training set\")\n",
        "print(\"--------------------------------------\")\n",
        "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
        "print(\"R^2: {}\".format(r2))\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyWMdPgW2Vbi",
        "colab_type": "text"
      },
      "source": [
        "## Supervised Learning: Classification Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1C6AENP2Vbk",
        "colab_type": "text"
      },
      "source": [
        "To visualize the workings of machine learning algorithms, it is often helpful to study two-dimensional or one-dimensional data, that is, data with only one or two features. While in practice, datasets usually have many more features, it is hard to plot high-dimensional data on two-dimensional screens.\n",
        "\n",
        "We will illustrate some very simple examples before we move on to more \"real world\" data sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AnrJlT12Vbl",
        "colab_type": "text"
      },
      "source": [
        "First, we will look at a two class classification problems in two dimensions. We use the synthetic data generated by the ``make_blobs`` function, which generates clusters of points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4xwLLaY2Vbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 15\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(centers=2, random_state=0)\n",
        "\n",
        "print('X ~ n_samples x n_features:', X.shape)\n",
        "print('y ~ n_samples:', y.shape)\n",
        "\n",
        "print('\\nFirst 5 samples:\\n', X[:5, :])\n",
        "print('\\nFirst 5 labels:', y[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOQrD-Ks2Vbq",
        "colab_type": "text"
      },
      "source": [
        "As the data is two-dimensional, we can plot each sample as a point in a two-dimensional coordinate system, with the first feature being the x-axis and the second feature being the y-axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5okevFs2Vbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 16\n",
        "\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], \n",
        "            c='blue', s=40, label='0')\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], \n",
        "            c='red', s=40, label='1', marker='s')\n",
        "\n",
        "plt.xlabel('first feature')\n",
        "plt.ylabel('second feature')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvbtUZNK2Vbw",
        "colab_type": "text"
      },
      "source": [
        "Again, we want to split our data into a training set and a test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBRVLvgP2Vbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 17\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=1234,\n",
        "                                                    stratify=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m9zSvtt2Vbz",
        "colab_type": "text"
      },
      "source": [
        "Every algorithm is exposed in scikit-learn via an ''Estimator'' object. (All models in scikit-learn have a very consistent interface). For instance, we first import the logistic regression class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u5uM2xKM-np",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 18\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBM-79MH2Vb3",
        "colab_type": "text"
      },
      "source": [
        "Next, we instantiate the estimator object. Practically speaking, this is how we begin implementing each machine learning technique.\n",
        "\n",
        "###  Logistic Regression\n",
        "\n",
        "\n",
        "Logistic Regression is used when the dependent variable(target) is categorical.\n",
        "For example,\n",
        "\n",
        "\n",
        "*  To predict whether an email is spam (1) or (0)\n",
        "*   Whether the tumor is malignant (1) or not (0)\n",
        "\n",
        "![alt text](https://miro.medium.com/max/288/0*myFbLX9TVrfNbMDz.png)\n",
        "\n",
        "\n",
        "linear regression is not suitable for classification problem. Linear regression is unbounded, and this brings logistic regression into picture. Their value strictly ranges from 0 to 1.\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/2320/0*j4b6G61h6FGvaS16.jpg) \n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1280/0*J4lgakQXSn4muaDQ.jpg)\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/818/1*ueEwU1dE0Yu-KpMJanf9AQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HlKxfIp2Vb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 19\n",
        "\n",
        "classifier = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfHUdyRp2Vb7",
        "colab_type": "text"
      },
      "source": [
        "How many instances are in the training set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1mphOh42Vb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 20\n",
        "\n",
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKk48B562VcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 21\n",
        "\n",
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7e7BDbw2VcI",
        "colab_type": "text"
      },
      "source": [
        "To build the model from our data, that is, to learn how to classify new points, we call the ``fit`` method with the training data and corresponding training labels (the desired output for the training data point):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbirTEyZ2VcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 22\n",
        "\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tayouVCG2VcM",
        "colab_type": "text"
      },
      "source": [
        "(Some estimator methods such as `fit` return `self` by default. Thus, after executing the code snippet above, you will see the default parameters of this particular instance of `LogisticRegression`. Another way of retrieving the estimator's ininitialization parameters is to execute `classifier.get_params()`, which returns a parameter dictionary.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_a7ZvB32VcN",
        "colab_type": "text"
      },
      "source": [
        "We can then apply the model to unseen data and use the model to predict the estimated outcome using the ``predict`` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaxCcptv2VcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 23\n",
        "\n",
        "prediction = classifier.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe_gkWGL2VcQ",
        "colab_type": "text"
      },
      "source": [
        "We can compare these against the true labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kbQiOic2VcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 24\n",
        "\n",
        "print('prediction:', prediction)\n",
        "print('true labels:', y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2RvkwaX2VcW",
        "colab_type": "text"
      },
      "source": [
        "We can evaluate our classifier quantitatively by measuring what fraction of predictions is correct. This is called **accuracy**. There is a convenience function, ``score``, that all scikit-learn classifiers have to compute this directly from the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vUXXk3G2VcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 25\n",
        "\n",
        "classifier.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcr0DZvC2Vce",
        "colab_type": "text"
      },
      "source": [
        "It is often helpful to compare the generalization performance (on the test set) to the performance on the training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV5bK-BJ2Vcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 26\n",
        "\n",
        "classifier.score(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFN8Hk1pXb03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## accuracy on iris\n",
        "classifier.fit(train_X, train_y)\n",
        "classifier.score(test_X, test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1XswhmF2Vcj",
        "colab_type": "text"
      },
      "source": [
        "LogisticRegression is a so-called linear model,\n",
        "that means it will create a decision that is linear in the input space. In 2D, this simply means it finds a line to separate the blue from the red:\n",
        "\n",
        "\n",
        "p≥0.5,class=1\n",
        "p<0.5,class=0\n",
        " \n",
        "For example, if our threshold was .5 and our prediction function returned .7, we would classify this observation as positive. If our prediction was .2 we would classify the observation as negative. For logistic regression with multiple classes we could select the class with the highest predicted probability.\n",
        "\n",
        "\n",
        "![alt text](https://ml-cheatsheet.readthedocs.io/en/latest/_images/logistic_regression_sigmoid_w_threshold.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4Z5IM4P2Vck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 27\n",
        "\n",
        "def plot_2d_separator(classifier, X, fill=False, ax=None, eps=None):\n",
        "    if eps is None:\n",
        "        eps = X.std() / 2.\n",
        "    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n",
        "    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n",
        "    xx = np.linspace(x_min, x_max, 100)\n",
        "    yy = np.linspace(y_min, y_max, 100)\n",
        "\n",
        "    X1, X2 = np.meshgrid(xx, yy)\n",
        "    X_grid = np.c_[X1.ravel(), X2.ravel()]\n",
        "    try:\n",
        "        decision_values = classifier.decision_function(X_grid)\n",
        "        levels = [0]\n",
        "        fill_levels = [decision_values.min(), 0, decision_values.max()]\n",
        "    except AttributeError:\n",
        "        # no decision_function\n",
        "        decision_values = classifier.predict_proba(X_grid)[:, 1]\n",
        "        levels = [.5]\n",
        "        fill_levels = [0, .5, 1]\n",
        "\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    if fill:\n",
        "        ax.contourf(X1, X2, decision_values.reshape(X1.shape),\n",
        "                    levels=fill_levels, colors=['blue', 'red'])\n",
        "    else:\n",
        "        ax.contour(X1, X2, decision_values.reshape(X1.shape), levels=levels,\n",
        "                   colors=\"black\")\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], \n",
        "            c='blue', s=40, label='0')\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], \n",
        "            c='red', s=40, label='1', marker='s')\n",
        "\n",
        "plt.xlabel(\"first feature\")\n",
        "plt.ylabel(\"second feature\")\n",
        "plot_2d_separator(classifier, X)\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGu9vdQwY3py",
        "colab_type": "text"
      },
      "source": [
        "Iteration while optimising logistic regression cost function\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1600/1*PkEl-8DBQa-xEft_tacXLQ.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUJxW8vO2Vco",
        "colab_type": "text"
      },
      "source": [
        "**Estimated parameters**: All the estimated model parameters are attributes of the estimator object ending by an underscore. Here, these are the coefficients and the offset of the line:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr-M_AOT2Vcp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6a8c0d05-e03d-41fa-a08f-0ddc32df3aed"
      },
      "source": [
        "## CODE CELL 28\n",
        "\n",
        "print(classifier.coef_)\n",
        "print(classifier.intercept_)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.38092515 -1.49993172]]\n",
            "[1.54995538]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeSAs5Co2Vcr",
        "colab_type": "text"
      },
      "source": [
        "### Another classifier: K Nearest Neighbors\n",
        "\n",
        "Another popular and easy to understand classifier is K nearest neighbors (kNN).  It has one of the simplest learning strategies: given a new, unknown observation, look up in your reference database which ones have the closest features and assign the predominant class.\n",
        "\n",
        "You can find nice intutative visulization demo of KNN from stanford university on the following link\n",
        "http://vision.stanford.edu/teaching/cs231n-demos/knn/\n",
        "\n",
        "\n",
        "![alt text](https://danielmoralesp.gitbooks.io/inteligencia-artificial/assets/nearest_neighbor.gif)\n",
        "\n",
        "The interface is exactly the same as for ``LogisticRegression above``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32iLkYL-2Vct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 29\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHsSPCwh2Vcz",
        "colab_type": "text"
      },
      "source": [
        "This time we set a parameter of the KNeighborsClassifier to tell it we want to look at three nearest neighbors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-tPBfqF2Vc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 30\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIGXZ5d92Vc_",
        "colab_type": "text"
      },
      "source": [
        "We fit the model with our training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0DfjlZd2VdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 31\n",
        "\n",
        "knn.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcZgPIzZ2VdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 32\n",
        "\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], \n",
        "            c='blue', s=40, label='0')\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], \n",
        "            c='red', s=40, label='1', marker='s')\n",
        "\n",
        "plt.xlabel(\"first feature\")\n",
        "plt.ylabel(\"second feature\")\n",
        "plot_2d_separator(knn, X)\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgxLeGL82VdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 33\n",
        "\n",
        "knn.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjQwRN5l2VdI",
        "colab_type": "text"
      },
      "source": [
        "Let's apply the KNeighborsClassifier to the iris dataset. How does accuracy change with different values of ``n_neighbors``?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc4k1ePe2VdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 34\n",
        "\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.20,\n",
        "                                                    random_state=42)\n",
        "\n",
        "scores = []\n",
        "k_values = np.arange(1, 10)\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    scores.append(knn.score(X_test, y_test))\n",
        "    \n",
        "plt.plot(k_values, scores)\n",
        "plt.xlabel('Num. neighbors')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "print(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9-y79KFYhCg",
        "colab_type": "text"
      },
      "source": [
        "### Another classifier: Naive Bayes Classifier\n",
        "\n",
        "Naive Bayes Classifier use Bayes theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/402/1*6dmvRYysiU5PwWIcHRdKVw.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw2HURnpj9KB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "model = GaussianNB()\n",
        "\n",
        "# Train the model using the training sets\n",
        "model.fit(train_X, train_y)\n",
        "\n",
        "model.score(test_X, test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJQZP1Po2VdL",
        "colab_type": "text"
      },
      "source": [
        "Note that changing the ``random_state`` value in `train_test_split()` will give you a different accuracy profile. This inconsistency begs the question, what other metrics besides accuracy are available for comparing model performance?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkCF3Yay2VdM",
        "colab_type": "text"
      },
      "source": [
        "### Model Evaluation and Scoring Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAk175Vl2VdN",
        "colab_type": "text"
      },
      "source": [
        "The default scores in scikit-learn are ``accuracy`` for classification, which is the fraction of correctly classified samples, and ``r2`` for regression, with is the coefficient of determination.\n",
        "\n",
        "These are reasonable default choices in many scenarious; however, depending on our task, these are not always the definitive or recommended choices.\n",
        "\n",
        "For example: Our data might be highly imbalance.\n",
        "\n",
        "Scikit-learn has many helpful methods in the ``sklearn.metrics`` module that can help us with model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0Nj0Az92VdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 35\n",
        "# Using k=3 and fitting/predicting on the iris dataset\n",
        "\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.50,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y)\n",
        "\n",
        "classifier = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)\n",
        "y_test_pred = classifier.predict(X_test)\n",
        "print(\"Accuracy: {}\".format(classifier.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qD6pICA2VdS",
        "colab_type": "text"
      },
      "source": [
        "Here, we predicted 92% of samples correctly. For multi-class problems, it is often interesting to know which of the classes are hard to predict, and which are easy, or which classes get confused. One way to get more information about misclassifications is the ``confusion_matrix``, which shows for each true class, how frequent a given predicted outcome is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOtinInG2VdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 36\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_test_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL_6yjW02VdV",
        "colab_type": "text"
      },
      "source": [
        "A plot is sometimes more readable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMaIKLYh2VdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 37\n",
        "\n",
        "plt.matshow(confusion_matrix(y_test, y_test_pred), cmap=\"Blues\")\n",
        "plt.colorbar(shrink=0.8)\n",
        "plt.xticks(range(3))\n",
        "plt.yticks(range(3))\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.ylabel(\"True label\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrsTCFKV2VdZ",
        "colab_type": "text"
      },
      "source": [
        "We can see that most entries are on the diagonal, which means that we predicted nearly all samples correctly. The off-diagonal entries show us that some 2s were classified as 1s. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um_dUsNs2Vda",
        "colab_type": "text"
      },
      "source": [
        "Another useful function is the ``classification_report`` which provides precision, recall, fscore and support for all classes.\n",
        "With TP, FP, TN, FN standing for \"true positive\", \"false positive\", \"true negative\" and \"false negative\" repectively (follow this link for more explanation: https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABaGFOXH2Vdb",
        "colab_type": "text"
      },
      "source": [
        "Precision = TP / (TP + FP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OYgKGu-2Vdd",
        "colab_type": "text"
      },
      "source": [
        "Precision is a measure of how many of the predictions for a class actually belong to that class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkTaJoSB2Vde",
        "colab_type": "text"
      },
      "source": [
        "Recall = TP / (TP + FN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZdDK58W2Vdf",
        "colab_type": "text"
      },
      "source": [
        "Recall is how many of the true positives were recovered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HqFIqR12Vdg",
        "colab_type": "text"
      },
      "source": [
        "F1-score is the geometric average of precision and recall:\n",
        "\n",
        "F1 = 2 x (precision x recall) / (precision + recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TtBUDqL2Vdi",
        "colab_type": "text"
      },
      "source": [
        "The values of all these values above are in the closed interval [0, 1], where 1 means a perfect score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ3AOAMH2Vdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 38\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyhW4cXZ2Vdq",
        "colab_type": "text"
      },
      "source": [
        "These metrics are helpful in two particular cases that come up often in practice:\n",
        "1. Imbalanced classes, that is, one class might be much more frequent than the other.\n",
        "2. Asymmetric costs, that is, one kind of error is much more \"costly\" than the other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI6GXsCe2Vdr",
        "colab_type": "text"
      },
      "source": [
        "There are several other metrics which could be used for comparing model performance; see the `sklearn.metrics` [module documentation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiKA_dLl2Vds",
        "colab_type": "text"
      },
      "source": [
        "## Unsupervised Learning Example: K-Means Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6Io4L492Vdw",
        "colab_type": "text"
      },
      "source": [
        "Clustering is the task of gathering samples into groups of similar\n",
        "samples according to some predefined similarity or distance (dissimilarity)\n",
        "measure, such as the Euclidean distance.\n",
        "\n",
        "![alt text](https://www.jeremyjordan.me/content/images/2016/12/kmeans.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfjSfl8Q2Vdx",
        "colab_type": "text"
      },
      "source": [
        "Here, we will use one of the simplest clustering algorithms, K-means.\n",
        "This is an iterative algorithm which searches for a pre-specified number of cluster\n",
        "centers such that the distance from each point to its cluster is\n",
        "minimized. The standard implementation of K-means uses the Euclidean distance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L15gOsKO2Vdy",
        "colab_type": "text"
      },
      "source": [
        "For this task, we will use the iris dataset - remember, it does come with labels in the ``target`` array, but we can perform the clustering on the ``data`` array and ignore the true labels for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP-Z8Ztc2Vdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 39\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QZYZrQX2Vd1",
        "colab_type": "text"
      },
      "source": [
        "We can get the cluster labels either by calling fit and then accessing the \n",
        "``labels_`` attribute of the K means estimator, or by calling ``fit_predict``.\n",
        "Either way, the result contains the ID of the cluster that each point is assigned to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW2O9AzZ2Vd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 40\n",
        "# X represents the data array\n",
        "\n",
        "clusters = kmeans.fit_predict(X)\n",
        "clusters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6RLHVjA2Vd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 41\n",
        "# What does the label/target array look like again?\n",
        "\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlllaYj-2Vd6",
        "colab_type": "text"
      },
      "source": [
        "Even though it appears that we recovered the partitioning of the data into clusters with some degree of accuracy, the cluster IDs we assigned were arbitrary (i.e. a cluster label of \"0\" may or may not correspond to the label \"0\" in the `target` array - again, we only used the `data` array for training the clustering algorithm). Therefore, if we want to compared our predicted labels with the true labels, we must use a different scoring metric, such as ``adjusted_rand_score``, which is invariant to permutations of the labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQf-Zmbz2Vd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 42\n",
        "\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "adjusted_rand_score(y, clusters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geFHu3Yh2VeB",
        "colab_type": "text"
      },
      "source": [
        "#### What value of \"k\" do we use?\n",
        "\n",
        "In real-world datasets, the \"true labels\" are often unknown. In that case, how can we pre-specify the number of clusters, and what metrics can we use to compare different models?\n",
        "\n",
        "The most important consideration is your knowledge (or a subject-matter expert's knowledge) of the data and problem at hand. In addition, there is a rule-of-thumb approach called the Elbow method which can help in finding the optimal number of clusters. The Elbow method plots the inertia, or the sum of squared distances between samples and their corresponding cluster centers, against the number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYAnkmWe2VeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 43\n",
        "\n",
        "inertia = []\n",
        "for i in range(1, 11):\n",
        "    km = KMeans(n_clusters=i, \n",
        "                random_state=0)\n",
        "    km.fit(X)\n",
        "    inertia.append(km.inertia_)\n",
        "\n",
        "plt.plot(range(1, 11), inertia, marker='o')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Within-cluster sum-of-squares')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QooPnOJE2VeF",
        "colab_type": "text"
      },
      "source": [
        "Then, we pick the value that resembles the \"pit of an elbow.\" As we can see, this would be k=3 in this case, which makes sense given our knowledge of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZsX71gz2VeG",
        "colab_type": "text"
      },
      "source": [
        "Another method of cluster model evaluation is called the silhouette coefficient or the silhouette score. Silhouette refers to a method of interpretation and validation of consistency within clusters of data. The technique provides a succinct graphical representation of how well each object has been classified. A higher silhouette score indicates more well-defined clusters; that is, the clusters have less overlap between them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjThMLf32VeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CODE CELL 44\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "def get_silhouette_score(data, model):\n",
        "    s_score = silhouette_score(data, model.labels_)\n",
        "    return s_score\n",
        "\n",
        "ss = []\n",
        "for i in range(2, 11):\n",
        "    km = KMeans(n_clusters=i, \n",
        "                random_state=0)\n",
        "    km_model = km.fit(X)\n",
        "    ss.append(get_silhouette_score(X, km_model))\n",
        "\n",
        "plt.plot(range(2, 11), ss, 'mo-')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4uhRoFT2VeL",
        "colab_type": "text"
      },
      "source": [
        "Again, since we know we're working with data from three distinct species in this example, we don't need to go through this evaluation. However, it can be helpful when evaluating an unlabelled dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHK8QwhG2VeP",
        "colab_type": "text"
      },
      "source": [
        "## A recap on Scikit-learn's estimator interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGWyzD8u2VeQ",
        "colab_type": "text"
      },
      "source": [
        "Scikit-learn strives to have a uniform interface across all methods. Given a scikit-learn *estimator*\n",
        "object named `model`, the following methods are available (not all for each model):\n",
        "\n",
        "- Available in **all Estimators**\n",
        "  + `model.fit()` : fit training data. For supervised learning applications,\n",
        "    this accepts two arguments: the data `X` and the labels `y` (e.g. `model.fit(X, y)`).\n",
        "    For unsupervised learning applications, `fit` takes only a single argument,\n",
        "    the data `X` (e.g. `model.fit(X)`).\n",
        "- Available in **supervised estimators**\n",
        "  + `model.predict()` : given a trained model, predict the label of a new set of data.\n",
        "    This method accepts one argument, the new data `X_new` (e.g. `model.predict(X_new)`),\n",
        "    and returns the learned label for each object in the array.\n",
        "  + `model.predict_proba()` : For classification problems, some estimators also provide\n",
        "    this method, which returns the probability that a new observation has each categorical label.\n",
        "    In this case, the label with the highest probability is returned by `model.predict()`.\n",
        "  + `model.decision_function()` : For classification problems, some estimators provide an uncertainty estimate that is not a probability. For binary classification, a decision_function >= 0 means the positive class will be predicted, while < 0 means the negative class.\n",
        "  + `model.score()` : for classification or regression problems, most (all?) estimators implement\n",
        "    a score method.  Scores are between 0 and 1, with a larger score indicating a better fit. For classifiers, the `score` method computes the prediction accuracy. For regressors, `score` computes the coefficient of determination (R<sup>2</sup>) of the prediction.\n",
        "  + `model.transform()` : For feature selection algorithms, this will reduce the dataset to the selected features. For some classification and regression models such as some linear models and random forests, this method reduces the dataset to the most informative features. These classification and regression models can therefore also be used as feature selection methods.\n",
        "  \n",
        "- Available in **unsupervised estimators**\n",
        "  + `model.transform()` : given an unsupervised model, transform new data into the new basis.\n",
        "    This also accepts one argument `X_new`, and returns the new representation of the data based\n",
        "    on the unsupervised model.\n",
        "  + `model.fit_transform()` : some estimators implement this method,\n",
        "    which more efficiently performs a fit and a transform on the same input data.\n",
        "  + `model.predict()` : for clustering algorithms, the predict method will produce cluster labels for new data points. Not all clustering methods have this functionality.\n",
        "  + `model.predict_proba()` : Gaussian mixture models (GMMs) provide the probability for each point to be generated by a given mixture component.\n",
        "  + `model.score()` : Density models like KDE and GMMs provide the likelihood of the data under the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg7Kq-yW2VeS",
        "colab_type": "text"
      },
      "source": [
        "## And there's more..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU4NNCTP2VeT",
        "colab_type": "text"
      },
      "source": [
        "There are many more machine learning algorithms that scikit-learn covers, from support vector machines, to random forests, to neural network models. If you are interested in learning more, check out the documentation here: http://scikit-learn.org/stable/user_guide.html. The source tutorial for this notebook is also excellent - I highly recommend browsing through the notebooks and/or watching the lecture if you have time. See the link below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je_dMu0i2VeU",
        "colab_type": "text"
      },
      "source": [
        "*Reference*:\n",
        "\n",
        "A. Gramfort and A. Mueller, *Scipy 2017 sklearn*, (2017), GitHub Repository, https://github.com/amueller/scipy-2017-sklearn.\n",
        "\n",
        "https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSMN8UF72VeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}